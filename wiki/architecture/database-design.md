# Database Design

Comprehensive overview of the OmniDev Supreme database architecture, designed to support 29 AI agents with unified memory, session management, and real-time data processing.

## üóÑÔ∏è Database Architecture Overview

OmniDev Supreme employs a hybrid database architecture combining three specialized storage systems:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  üß† Unified Memory System                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                üîç Vector Database                       ‚îÇ
    ‚îÇ                   (FAISS)                              ‚îÇ
    ‚îÇ           Semantic Search & Embeddings                 ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ               üìä Relational Database                    ‚îÇ
    ‚îÇ                  (SQLite)                              ‚îÇ
    ‚îÇ          Structured Data & Relationships               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                ‚ö° Session Storage                       ‚îÇ
    ‚îÇ                   (Redis)                              ‚îÇ
    ‚îÇ            Fast Access & Caching                       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîç Vector Database (FAISS)

### **Purpose**
Semantic search and similarity matching for code, documentation, and knowledge retrieval.

### **Technology Stack**
- **FAISS**: Facebook AI Similarity Search
- **Embeddings**: OpenAI text-embedding-ada-002
- **Storage**: Binary index files with metadata

### **Schema Design**

#### **Vector Index Structure**
```python
class VectorIndex:
    """Vector index for semantic search."""
    
    def __init__(self, dimension: int = 1536):
        self.dimension = dimension
        self.index = faiss.IndexFlatIP(dimension)  # Inner product for similarity
        self.metadata: Dict[int, Dict[str, Any]] = {}
        self.id_to_vector_id: Dict[str, int] = {}
    
    def add_vector(self, memory_id: str, vector: np.ndarray, metadata: Dict[str, Any]):
        """Add vector to index with metadata."""
        vector_id = self.index.ntotal
        self.index.add(vector.reshape(1, -1))
        self.metadata[vector_id] = metadata
        self.id_to_vector_id[memory_id] = vector_id
    
    def search(self, query_vector: np.ndarray, k: int = 10) -> List[Dict[str, Any]]:
        """Search for similar vectors."""
        scores, indices = self.index.search(query_vector.reshape(1, -1), k)
        
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx != -1:  # Valid result
                results.append({
                    'score': float(score),
                    'metadata': self.metadata[idx],
                    'rank': i + 1
                })
        
        return results
```

#### **Embedding Strategy**
```python
class EmbeddingManager:
    """Manage text embeddings for vector search."""
    
    def __init__(self, model_name: str = "text-embedding-ada-002"):
        self.model_name = model_name
        self.client = OpenAI()
        self.cache: Dict[str, np.ndarray] = {}
    
    async def get_embedding(self, text: str) -> np.ndarray:
        """Get embedding for text with caching."""
        text_hash = hashlib.md5(text.encode()).hexdigest()
        
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        response = await self.client.embeddings.create(
            model=self.model_name,
            input=text
        )
        
        embedding = np.array(response.data[0].embedding)
        self.cache[text_hash] = embedding
        
        return embedding
    
    def preprocess_text(self, text: str) -> str:
        """Preprocess text for embedding."""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Truncate to model limits (8192 tokens for ada-002)
        if len(text) > 8000:  # Conservative limit
            text = text[:8000]
        
        return text.strip()
```

### **Vector Storage Categories**

#### **1. Code Embeddings**
```python
class CodeEmbedding:
    """Code-specific embedding with metadata."""
    
    memory_id: str
    content: str
    language: str
    file_path: Optional[str]
    function_name: Optional[str]
    class_name: Optional[str]
    complexity_score: float
    tags: List[str]
    agent_generated: bool
    timestamp: datetime
```

#### **2. Knowledge Embeddings**
```python
class KnowledgeEmbedding:
    """Knowledge-specific embedding with metadata."""
    
    memory_id: str
    content: str
    topic: str
    expertise_level: str  # beginner, intermediate, advanced
    source: str  # agent, documentation, user
    confidence_score: float
    related_concepts: List[str]
    timestamp: datetime
```

#### **3. Task Embeddings**
```python
class TaskEmbedding:
    """Task-specific embedding with metadata."""
    
    memory_id: str
    task_description: str
    task_type: str
    agent_id: str
    complexity: str
    success_rate: float
    execution_time: float
    timestamp: datetime
```

### **Search Strategies**

#### **Semantic Search**
```python
async def semantic_search(
    query: str,
    memory_type: Optional[MemoryType] = None,
    limit: int = 10,
    threshold: float = 0.7
) -> List[Dict[str, Any]]:
    """Perform semantic search across embeddings."""
    
    # Get query embedding
    query_embedding = await self.embedding_manager.get_embedding(query)
    
    # Search in appropriate index
    if memory_type == MemoryType.CODE:
        results = self.code_index.search(query_embedding, limit * 2)
    elif memory_type == MemoryType.KNOWLEDGE:
        results = self.knowledge_index.search(query_embedding, limit * 2)
    else:
        # Search across all indices
        results = self._search_all_indices(query_embedding, limit * 2)
    
    # Filter by threshold and limit
    filtered_results = [
        result for result in results
        if result['score'] >= threshold
    ]
    
    return filtered_results[:limit]
```

#### **Hybrid Search**
```python
async def hybrid_search(
    query: str,
    filters: Dict[str, Any],
    limit: int = 10
) -> List[Dict[str, Any]]:
    """Combine semantic and metadata filtering."""
    
    # Get semantic results
    semantic_results = await self.semantic_search(query, limit=limit * 3)
    
    # Apply metadata filters
    filtered_results = []
    for result in semantic_results:
        metadata = result['metadata']
        
        # Apply filters
        if self._matches_filters(metadata, filters):
            filtered_results.append(result)
    
    return filtered_results[:limit]
```

## üìä Relational Database (SQLite)

### **Purpose**
Structured data storage for agents, tasks, projects, and system metadata.

### **Technology Stack**
- **SQLite**: Lightweight, file-based database
- **SQLAlchemy**: ORM for Python
- **Alembic**: Database migrations

### **Schema Design**

#### **1. Agent Management**

```sql
-- Agents table
CREATE TABLE agents (
    id VARCHAR(50) PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    agent_type VARCHAR(50) NOT NULL,
    system_name VARCHAR(50) NOT NULL,
    description TEXT,
    capabilities TEXT,  -- JSON array
    priority INTEGER DEFAULT 5,
    max_concurrent_tasks INTEGER DEFAULT 1,
    timeout_seconds INTEGER DEFAULT 300,
    model_requirements TEXT,  -- JSON array
    tags TEXT,  -- JSON array
    status VARCHAR(20) DEFAULT 'idle',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Agent statistics
CREATE TABLE agent_stats (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    agent_id VARCHAR(50) NOT NULL,
    tasks_completed INTEGER DEFAULT 0,
    tasks_failed INTEGER DEFAULT 0,
    total_execution_time REAL DEFAULT 0.0,
    total_tokens_used INTEGER DEFAULT 0,
    average_response_time REAL DEFAULT 0.0,
    success_rate REAL DEFAULT 0.0,
    date DATE DEFAULT CURRENT_DATE,
    FOREIGN KEY (agent_id) REFERENCES agents(id)
);

-- Agent performance metrics
CREATE TABLE agent_performance (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    agent_id VARCHAR(50) NOT NULL,
    task_type VARCHAR(50),
    execution_time REAL,
    tokens_used INTEGER,
    success BOOLEAN,
    error_message TEXT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (agent_id) REFERENCES agents(id)
);
```

#### **2. Task Management**

```sql
-- Tasks table
CREATE TABLE tasks (
    id VARCHAR(50) PRIMARY KEY,
    content TEXT NOT NULL,
    task_type VARCHAR(50) NOT NULL,
    complexity VARCHAR(20) DEFAULT 'medium',
    priority INTEGER DEFAULT 5,
    agent_id VARCHAR(50),
    status VARCHAR(20) DEFAULT 'pending',
    progress INTEGER DEFAULT 0,
    result TEXT,  -- JSON
    error_message TEXT,
    context TEXT,  -- JSON
    constraints TEXT,  -- JSON
    session_id VARCHAR(50),
    project_id VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    estimated_completion TIMESTAMP,
    FOREIGN KEY (agent_id) REFERENCES agents(id),
    FOREIGN KEY (project_id) REFERENCES projects(id)
);

-- Task dependencies
CREATE TABLE task_dependencies (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    task_id VARCHAR(50) NOT NULL,
    depends_on_task_id VARCHAR(50) NOT NULL,
    dependency_type VARCHAR(20) DEFAULT 'sequential',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (task_id) REFERENCES tasks(id),
    FOREIGN KEY (depends_on_task_id) REFERENCES tasks(id)
);

-- Task workflows
CREATE TABLE workflows (
    id VARCHAR(50) PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    definition TEXT NOT NULL,  -- JSON workflow definition
    status VARCHAR(20) DEFAULT 'active',
    created_by VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### **3. Project Management**

```sql
-- Projects table
CREATE TABLE projects (
    id VARCHAR(50) PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    language VARCHAR(50),
    framework VARCHAR(50),
    status VARCHAR(20) DEFAULT 'active',
    settings TEXT,  -- JSON
    created_by VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Project files
CREATE TABLE project_files (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    project_id VARCHAR(50) NOT NULL,
    file_path VARCHAR(500) NOT NULL,
    content TEXT,
    language VARCHAR(50),
    size INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (project_id) REFERENCES projects(id)
);

-- Project statistics
CREATE TABLE project_stats (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    project_id VARCHAR(50) NOT NULL,
    files_count INTEGER DEFAULT 0,
    lines_of_code INTEGER DEFAULT 0,
    tasks_completed INTEGER DEFAULT 0,
    agents_used INTEGER DEFAULT 0,
    last_activity TIMESTAMP,
    FOREIGN KEY (project_id) REFERENCES projects(id)
);
```

#### **4. Memory System**

```sql
-- Memory entries
CREATE TABLE memory_entries (
    id VARCHAR(50) PRIMARY KEY,
    content TEXT NOT NULL,
    memory_type VARCHAR(50) NOT NULL,
    priority INTEGER DEFAULT 5,
    metadata TEXT,  -- JSON
    tags TEXT,  -- JSON array
    agent_id VARCHAR(50),
    session_id VARCHAR(50),
    project_id VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (agent_id) REFERENCES agents(id),
    FOREIGN KEY (project_id) REFERENCES projects(id)
);

-- Memory relationships
CREATE TABLE memory_relationships (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_memory_id VARCHAR(50) NOT NULL,
    target_memory_id VARCHAR(50) NOT NULL,
    relationship_type VARCHAR(50) NOT NULL,
    strength REAL DEFAULT 1.0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (source_memory_id) REFERENCES memory_entries(id),
    FOREIGN KEY (target_memory_id) REFERENCES memory_entries(id)
);

-- Memory access patterns
CREATE TABLE memory_access (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    memory_id VARCHAR(50) NOT NULL,
    agent_id VARCHAR(50),
    session_id VARCHAR(50),
    access_type VARCHAR(20) NOT NULL,  -- read, write, update, delete
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (memory_id) REFERENCES memory_entries(id),
    FOREIGN KEY (agent_id) REFERENCES agents(id)
);
```

#### **5. System Monitoring**

```sql
-- System health
CREATE TABLE system_health (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    component VARCHAR(50) NOT NULL,
    status VARCHAR(20) NOT NULL,
    metrics TEXT,  -- JSON
    message TEXT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- API usage
CREATE TABLE api_usage (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    endpoint VARCHAR(100) NOT NULL,
    method VARCHAR(10) NOT NULL,
    agent_id VARCHAR(50),
    session_id VARCHAR(50),
    response_time REAL,
    status_code INTEGER,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (agent_id) REFERENCES agents(id)
);

-- Error logs
CREATE TABLE error_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    component VARCHAR(50) NOT NULL,
    error_type VARCHAR(50) NOT NULL,
    message TEXT NOT NULL,
    stack_trace TEXT,
    agent_id VARCHAR(50),
    session_id VARCHAR(50),
    severity VARCHAR(20) DEFAULT 'error',
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (agent_id) REFERENCES agents(id)
);
```

### **Database Operations**

#### **SQLAlchemy Models**
```python
from sqlalchemy import Column, String, Integer, Float, Boolean, DateTime, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime

Base = declarative_base()

class Agent(Base):
    __tablename__ = 'agents'
    
    id = Column(String(50), primary_key=True)
    name = Column(String(100), nullable=False)
    agent_type = Column(String(50), nullable=False)
    system_name = Column(String(50), nullable=False)
    description = Column(Text)
    capabilities = Column(Text)  # JSON
    priority = Column(Integer, default=5)
    max_concurrent_tasks = Column(Integer, default=1)
    timeout_seconds = Column(Integer, default=300)
    model_requirements = Column(Text)  # JSON
    tags = Column(Text)  # JSON
    status = Column(String(20), default='idle')
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    tasks = relationship("Task", back_populates="agent")
    stats = relationship("AgentStats", back_populates="agent")
    performance = relationship("AgentPerformance", back_populates="agent")

class Task(Base):
    __tablename__ = 'tasks'
    
    id = Column(String(50), primary_key=True)
    content = Column(Text, nullable=False)
    task_type = Column(String(50), nullable=False)
    complexity = Column(String(20), default='medium')
    priority = Column(Integer, default=5)
    agent_id = Column(String(50), ForeignKey('agents.id'))
    status = Column(String(20), default='pending')
    progress = Column(Integer, default=0)
    result = Column(Text)  # JSON
    error_message = Column(Text)
    context = Column(Text)  # JSON
    constraints = Column(Text)  # JSON
    session_id = Column(String(50))
    project_id = Column(String(50), ForeignKey('projects.id'))
    created_at = Column(DateTime, default=datetime.utcnow)
    started_at = Column(DateTime)
    completed_at = Column(DateTime)
    estimated_completion = Column(DateTime)
    
    # Relationships
    agent = relationship("Agent", back_populates="tasks")
    project = relationship("Project", back_populates="tasks")
```

#### **Database Queries**
```python
class DatabaseManager:
    """Database operations manager."""
    
    def __init__(self, database_url: str):
        self.engine = create_engine(database_url)
        self.Session = sessionmaker(bind=self.engine)
    
    async def get_agent_by_id(self, agent_id: str) -> Optional[Agent]:
        """Get agent by ID."""
        with self.Session() as session:
            return session.query(Agent).filter(Agent.id == agent_id).first()
    
    async def get_available_agents(self, task_type: str = None) -> List[Agent]:
        """Get available agents for task execution."""
        with self.Session() as session:
            query = session.query(Agent).filter(Agent.status == 'idle')
            
            if task_type:
                # Filter by capabilities (JSON contains)
                query = query.filter(Agent.capabilities.contains(task_type))
            
            return query.order_by(Agent.priority.desc()).all()
    
    async def create_task(self, task_data: Dict[str, Any]) -> Task:
        """Create new task."""
        with self.Session() as session:
            task = Task(**task_data)
            session.add(task)
            session.commit()
            session.refresh(task)
            return task
    
    async def update_task_progress(
        self, task_id: str, progress: int, status: str = None
    ) -> bool:
        """Update task progress."""
        with self.Session() as session:
            task = session.query(Task).filter(Task.id == task_id).first()
            if task:
                task.progress = progress
                if status:
                    task.status = status
                session.commit()
                return True
            return False
    
    async def get_agent_performance(
        self, agent_id: str, days: int = 30
    ) -> Dict[str, Any]:
        """Get agent performance metrics."""
        with self.Session() as session:
            cutoff_date = datetime.utcnow() - timedelta(days=days)
            
            stats = session.query(AgentPerformance).filter(
                AgentPerformance.agent_id == agent_id,
                AgentPerformance.timestamp >= cutoff_date
            ).all()
            
            return {
                'total_tasks': len(stats),
                'success_rate': sum(1 for s in stats if s.success) / len(stats) if stats else 0,
                'average_execution_time': sum(s.execution_time for s in stats) / len(stats) if stats else 0,
                'total_tokens_used': sum(s.tokens_used for s in stats if s.tokens_used),
                'error_rate': sum(1 for s in stats if not s.success) / len(stats) if stats else 0
            }
```

## ‚ö° Session Storage (Redis)

### **Purpose**
Fast, temporary storage for session data, caching, and real-time communication.

### **Technology Stack**
- **Redis**: In-memory data structure store
- **Redis-py**: Python client for Redis
- **TTL**: Time-to-live for automatic cleanup

### **Data Structures**

#### **1. Session Data**
```python
# Session structure
session_data = {
    'session_id': 'session_123',
    'user_id': 'user_456',
    'created_at': '2024-12-15T10:30:00Z',
    'last_activity': '2024-12-15T10:45:00Z',
    'context': {
        'current_project': 'project_789',
        'active_agents': ['agent_001', 'agent_002'],
        'conversation_history': [...],
        'preferences': {...}
    },
    'temporary_data': {
        'draft_code': '...',
        'incomplete_tasks': [...],
        'search_results': [...]
    }
}

# Storage pattern
redis_client.setex(
    f'session:{session_id}',
    3600,  # 1 hour TTL
    json.dumps(session_data)
)
```

#### **2. Agent State Cache**
```python
# Agent state caching
agent_state = {
    'agent_id': 'agent_001',
    'status': 'busy',
    'current_task': 'task_123',
    'progress': 75,
    'estimated_completion': '2024-12-15T10:50:00Z',
    'performance_metrics': {
        'response_time': 2.3,
        'success_rate': 0.95,
        'tokens_used': 1500
    }
}

# Cache with shorter TTL for real-time updates
redis_client.setex(
    f'agent_state:{agent_id}',
    300,  # 5 minutes TTL
    json.dumps(agent_state)
)
```

#### **3. Task Queue**
```python
# Task queue using Redis lists
class TaskQueue:
    def __init__(self, redis_client):
        self.redis_client = redis_client
        self.queue_name = 'task_queue'
        self.processing_set = 'processing_tasks'
    
    async def enqueue_task(self, task: Dict[str, Any]):
        """Add task to queue."""
        task_json = json.dumps(task)
        await self.redis_client.lpush(self.queue_name, task_json)
    
    async def dequeue_task(self, agent_id: str) -> Optional[Dict[str, Any]]:
        """Get next task from queue."""
        task_json = await self.redis_client.brpop(self.queue_name, timeout=30)
        if task_json:
            task = json.loads(task_json[1])
            # Mark as processing
            await self.redis_client.sadd(self.processing_set, task['id'])
            return task
        return None
    
    async def complete_task(self, task_id: str):
        """Mark task as completed."""
        await self.redis_client.srem(self.processing_set, task_id)
```

#### **4. Real-time Communication**
```python
# WebSocket connection tracking
class WebSocketManager:
    def __init__(self, redis_client):
        self.redis_client = redis_client
        self.pubsub = redis_client.pubsub()
    
    async def register_connection(self, session_id: str, connection_id: str):
        """Register WebSocket connection."""
        await self.redis_client.sadd(f'ws_connections:{session_id}', connection_id)
        await self.redis_client.expire(f'ws_connections:{session_id}', 3600)
    
    async def broadcast_to_session(self, session_id: str, message: Dict[str, Any]):
        """Broadcast message to all connections in session."""
        channel = f'session:{session_id}'
        await self.redis_client.publish(channel, json.dumps(message))
    
    async def broadcast_agent_update(self, agent_id: str, update: Dict[str, Any]):
        """Broadcast agent status update."""
        channel = f'agent_updates:{agent_id}'
        await self.redis_client.publish(channel, json.dumps(update))
```

### **Caching Strategies**

#### **1. Memory Result Cache**
```python
class MemoryResultCache:
    """Cache for memory search results."""
    
    def __init__(self, redis_client, ttl: int = 1800):  # 30 minutes
        self.redis_client = redis_client
        self.ttl = ttl
    
    async def get_cached_result(self, query_hash: str) -> Optional[List[Dict[str, Any]]]:
        """Get cached search result."""
        cached = await self.redis_client.get(f'memory_search:{query_hash}')
        if cached:
            return json.loads(cached)
        return None
    
    async def cache_result(self, query_hash: str, results: List[Dict[str, Any]]):
        """Cache search results."""
        await self.redis_client.setex(
            f'memory_search:{query_hash}',
            self.ttl,
            json.dumps(results)
        )
```

#### **2. Agent Response Cache**
```python
class AgentResponseCache:
    """Cache for agent responses to similar tasks."""
    
    def __init__(self, redis_client, ttl: int = 7200):  # 2 hours
        self.redis_client = redis_client
        self.ttl = ttl
    
    async def get_cached_response(
        self, agent_id: str, task_hash: str
    ) -> Optional[Dict[str, Any]]:
        """Get cached agent response."""
        key = f'agent_response:{agent_id}:{task_hash}'
        cached = await self.redis_client.get(key)
        if cached:
            return json.loads(cached)
        return None
    
    async def cache_response(
        self, agent_id: str, task_hash: str, response: Dict[str, Any]
    ):
        """Cache agent response."""
        key = f'agent_response:{agent_id}:{task_hash}'
        await self.redis_client.setex(key, self.ttl, json.dumps(response))
```

## üìä Data Consistency & Synchronization

### **1. Cross-Database Synchronization**
```python
class DataSynchronizer:
    """Synchronize data across database systems."""
    
    def __init__(self, relational_db, vector_db, redis_client):
        self.relational_db = relational_db
        self.vector_db = vector_db
        self.redis_client = redis_client
    
    async def sync_memory_entry(self, memory_entry: Dict[str, Any]):
        """Synchronize memory entry across all systems."""
        memory_id = memory_entry['id']
        
        # Store in relational database
        await self.relational_db.store_memory_entry(memory_entry)
        
        # Generate and store embedding
        embedding = await self.generate_embedding(memory_entry['content'])
        await self.vector_db.store_embedding(memory_id, embedding, memory_entry)
        
        # Cache in Redis for fast access
        await self.redis_client.setex(
            f'memory:{memory_id}',
            3600,
            json.dumps(memory_entry)
        )
    
    async def sync_agent_status(self, agent_id: str, status: str):
        """Synchronize agent status across systems."""
        # Update in relational database
        await self.relational_db.update_agent_status(agent_id, status)
        
        # Update in Redis cache
        await self.redis_client.setex(
            f'agent_status:{agent_id}',
            300,
            status
        )
        
        # Broadcast to WebSocket connections
        await self.broadcast_agent_update(agent_id, {'status': status})
```

### **2. Transaction Management**
```python
class TransactionManager:
    """Manage transactions across multiple databases."""
    
    def __init__(self, relational_db, vector_db, redis_client):
        self.relational_db = relational_db
        self.vector_db = vector_db
        self.redis_client = redis_client
    
    async def execute_transaction(self, operations: List[Dict[str, Any]]):
        """Execute operations as a transaction."""
        # Start transaction log
        transaction_id = str(uuid.uuid4())
        await self.redis_client.setex(
            f'transaction:{transaction_id}',
            3600,
            json.dumps({'status': 'started', 'operations': operations})
        )
        
        try:
            # Execute all operations
            for operation in operations:
                await self._execute_operation(operation)
            
            # Mark transaction as completed
            await self.redis_client.setex(
                f'transaction:{transaction_id}',
                3600,
                json.dumps({'status': 'completed'})
            )
            
        except Exception as e:
            # Rollback operations
            await self._rollback_transaction(transaction_id, operations)
            raise
    
    async def _rollback_transaction(
        self, transaction_id: str, operations: List[Dict[str, Any]]
    ):
        """Rollback transaction operations."""
        # Implement rollback logic for each operation type
        for operation in reversed(operations):
            await self._rollback_operation(operation)
        
        # Mark transaction as rolled back
        await self.redis_client.setex(
            f'transaction:{transaction_id}',
            3600,
            json.dumps({'status': 'rolled_back'})
        )
```

## üîß Performance Optimization

### **1. Database Indexing**
```sql
-- Performance indexes
CREATE INDEX idx_agents_status ON agents(status);
CREATE INDEX idx_agents_type ON agents(agent_type);
CREATE INDEX idx_tasks_status ON tasks(status);
CREATE INDEX idx_tasks_agent_id ON tasks(agent_id);
CREATE INDEX idx_tasks_created_at ON tasks(created_at);
CREATE INDEX idx_memory_entries_type ON memory_entries(memory_type);
CREATE INDEX idx_memory_entries_created_at ON memory_entries(created_at);
CREATE INDEX idx_agent_performance_timestamp ON agent_performance(timestamp);
```

### **2. Query Optimization**
```python
class OptimizedQueries:
    """Optimized database queries."""
    
    @staticmethod
    async def get_agent_workload(session) -> Dict[str, int]:
        """Get current workload for all agents."""
        query = """
        SELECT a.id, a.name, COUNT(t.id) as active_tasks
        FROM agents a
        LEFT JOIN tasks t ON a.id = t.agent_id 
        WHERE a.status IN ('idle', 'busy') 
        AND (t.status IN ('pending', 'in_progress') OR t.id IS NULL)
        GROUP BY a.id, a.name
        ORDER BY active_tasks ASC
        """
        result = await session.execute(query)
        return {row.id: row.active_tasks for row in result}
    
    @staticmethod
    async def get_performance_summary(session, days: int = 7) -> Dict[str, Any]:
        """Get performance summary for all agents."""
        query = """
        SELECT 
            a.id,
            a.name,
            COUNT(ap.id) as total_tasks,
            AVG(ap.execution_time) as avg_execution_time,
            SUM(ap.tokens_used) as total_tokens,
            (SUM(CASE WHEN ap.success THEN 1 ELSE 0 END) * 100.0 / COUNT(ap.id)) as success_rate
        FROM agents a
        LEFT JOIN agent_performance ap ON a.id = ap.agent_id
        WHERE ap.timestamp >= datetime('now', '-{} days')
        GROUP BY a.id, a.name
        ORDER BY success_rate DESC
        """.format(days)
        
        result = await session.execute(query)
        return [dict(row) for row in result]
```

### **3. Connection Pooling**
```python
class DatabaseConnectionPool:
    """Connection pool management."""
    
    def __init__(self, database_url: str, pool_size: int = 10):
        self.engine = create_engine(
            database_url,
            pool_size=pool_size,
            max_overflow=20,
            pool_pre_ping=True,
            pool_recycle=3600
        )
        self.Session = sessionmaker(bind=self.engine)
    
    @contextmanager
    def get_session(self):
        """Get database session with automatic cleanup."""
        session = self.Session()
        try:
            yield session
            session.commit()
        except Exception:
            session.rollback()
            raise
        finally:
            session.close()
```

---

<div align="center">
  <p><strong>üóÑÔ∏è Robust database design for enterprise-scale AI platform</strong></p>
  <p>Three specialized storage systems working in harmony for optimal performance.</p>
  
  <a href="api-design.md">
    <strong>Explore API Design ‚Üí</strong>
  </a>
</div>

---

*Last updated: December 2024 | [Edit this page](https://github.com/meistro57/omnidev-supreme/edit/main/wiki/architecture/database-design.md)*